# Python Scraper to collect data from Wikipeda:

This python scraper collects data from wikipedia about the top cities in USA. 

## Getting Started

These instructions provide prerequisites necessary to run this program.

### Prerequisites

It needs Jupyter notebook to run the .ipynb script.

```
https://jupyter.readthedocs.io/en/latest/install.html
```

It needs pandas package:

```
pip install pandas

```

install wikipedia library

```
pip install wikipedia
```

## Running the Script:
We can either run the python script, or the jupyter script.
The different parts of the script are below:


###Step0:
Import Necessary libraries

###Step1: 
Read the html table from wikipedia and find DataFrame object: We read HTML tables into a list of DataFrame objects. It finds the table element, does the parsing and creates a DataFrame.


### Break down into end to end tests

Explain what these tests test and why

```
Give an example
```

### And coding style tests

Explain what these tests test and why

```
Give an example
```

## Deployment

Add additional notes about how to deploy this on a live system

## Built With

* [Dropwizard](http://www.dropwizard.io/1.0.2/docs/) - The web framework used
* [Maven](https://maven.apache.org/) - Dependency Management
* [ROME](https://rometools.github.io/rome/) - Used to generate RSS Feeds

## Contributing

Please read [CONTRIBUTING.md](https://gist.github.com/PurpleBooth/b24679402957c63ec426) for details on our code of conduct, and the process for submitting pull requests to us.

## Versioning

We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/your/project/tags). 

## Authors

* **Billie Thompson** - *Initial work* - [PurpleBooth](https://github.com/PurpleBooth)

See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Hat tip to anyone whose code was used
* Inspiration
* etc

